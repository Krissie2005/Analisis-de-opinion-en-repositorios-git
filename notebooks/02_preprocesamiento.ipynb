{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ea69b97",
   "metadata": {},
   "source": [
    "# Preprocesamiento del texto\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e354c0fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd #armar y guardar el CSV\n",
    "import re #Limpieza\n",
    "import unicodedata #Normaliza caracteres Unicode.\n",
    "import nltk \n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords # Recursos de NLTK (stopwords)\n",
    "stop_en = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebd6f33",
   "metadata": {},
   "source": [
    "\n",
    "**Limpieza clásica para texto (GitHub)**\n",
    "    - lower\n",
    "    - quitar URLs\n",
    "    - quitar acentos (por seguridad)\n",
    "    - dejar letras/espacios\n",
    "    - quitar stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7ecf66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpiar_texto(texto):\n",
    "    #Pasa a minúsculas\n",
    "    t = str(texto).lower() \n",
    "\n",
    "    # URLs\n",
    "    t = re.sub(r\"http\\S+|www\\S+\", \" \", t)\n",
    "\n",
    "    # Acentos (por si hay)\n",
    "    t = unicodedata.normalize(\"NFD\", t)\n",
    "    t = t.encode(\"ascii\", \"ignore\").decode(\"utf-8\")\n",
    "\n",
    "    # Solo letras\n",
    "    t = re.sub(r\"[^a-z\\s]\", \" \", t)\n",
    "\n",
    "    # Espacios extra\n",
    "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
    "\n",
    "    # Stopwords\n",
    "    palabras = [w for w in t.split() if w not in stop_en]\n",
    "    return \" \".join(palabras)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f501c4",
   "metadata": {},
   "source": [
    "Definir repos a procesar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34d42587",
   "metadata": {},
   "outputs": [],
   "source": [
    "repos_slugs = [\"auto1111_webui\", \"ytdlp\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e461085",
   "metadata": {},
   "source": [
    "**Iteración por repositorio**\n",
    "\n",
    "* Recorre cada slug en repos_slugs para procesar los repos de forma independiente.\n",
    "\n",
    "**Carga de datos**\n",
    "\n",
    "* Lee el archivo messages_raw.csv de cada repositorio.\n",
    "\n",
    "* Contiene issues y comentarios sin limpiar.\n",
    "\n",
    "**Construcción del texto base**\n",
    "\n",
    "* Rellena valores nulos en title y body.\n",
    "\n",
    "* Une title y body en una sola columna text.\n",
    "\n",
    "**Limpieza del texto**\n",
    "\n",
    "* Aplica la función limpiar_texto:\n",
    "\n",
    "1. pasa a minúsculas\n",
    "\n",
    "2. elimina URLs y acentos\n",
    "\n",
    "3. conserva solo letras y espacios\n",
    "\n",
    "4. elimina stopwords en inglés\n",
    "\n",
    "* El resultado se guarda en text_clean.\n",
    "\n",
    "**Eliminación de textos vacíos**\n",
    "\n",
    "* Se eliminan filas donde text_clean queda vacío. (Paso clave para evitar errores en TF-IDE)\n",
    "\n",
    "**Control de cambios**\n",
    "\n",
    "* Muestra cuántos registros había antes y después de la limpieza.\n",
    "\n",
    "**Guardado del resultado**\n",
    "\n",
    "* Guarda el archivo final como messages_clean.csv.\n",
    "\n",
    "* Dataset listo para análisis NLP posteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5aa271db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Repo auto1111_webui: antes=475, después=459, eliminados=16\n",
      "Guardado ../data/auto1111_webui/messages_clean.csv\n",
      "\n",
      "Repo ytdlp: antes=697, después=697, eliminados=0\n",
      "Guardado ../data/ytdlp/messages_clean.csv\n"
     ]
    }
   ],
   "source": [
    "for slug in repos_slugs:\n",
    "    ruta_in = f\"../data/{slug}/messages_raw.csv\" #ruta donde está el archivo generado en el file 1\n",
    "    df = pd.read_csv(ruta_in) #Carga el archivo en un DataFrame\n",
    " \n",
    "    # texto base = title + body - evitar valores nulos\n",
    "    df[\"title\"] = df[\"title\"].fillna(\"\")\n",
    "    df[\"body\"]  = df[\"body\"].fillna(\"\") #Esto reemplaza NaN por cadena vacía\n",
    "    df[\"text\"] = (df[\"title\"].astype(str) + \" \" + df[\"body\"].astype(str)).str.strip()\n",
    "    #Cada fila ahora tiene una columna text que contiene todo el contenido textual del mensaje\n",
    "    \n",
    "    # limpiar - llama a la función\n",
    "    df[\"text_clean\"] = df[\"text\"].apply(limpiar_texto)\n",
    "\n",
    "    # eliminar vacíos - importante para TF-IDF)\n",
    "    antes = df.shape[0] #Guarda cuántas filas había antes\n",
    "    df = df[df[\"text_clean\"].str.strip() != \"\"].copy() #quita filas que esten vacías\n",
    "    despues = df.shape[0] #Guarda cuántas filas quedaron\n",
    "\n",
    "    print(f\"\\nRepo {slug}: antes={antes}, después={despues}, eliminados={antes-despues}\")\n",
    "\n",
    "    ruta_out = f\"../data/{slug}/messages_clean.csv\" #Guarda el nuevo archivo limpio\n",
    "    df.to_csv(ruta_out, index=False)\n",
    "    print(\"Guardado\", ruta_out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
